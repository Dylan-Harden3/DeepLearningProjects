{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First load the training data"
      ],
      "metadata": {
        "id": "n-TiFNHkqk3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJOq4TgvntR3"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "train_input = pickle.load(open('Train_input', 'rb'))\n",
        "train_output = pickle.load(open( 'Train_output', 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets add [start] and [end] words to the output sequences"
      ],
      "metadata": {
        "id": "lBGBxz3bzEhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_pairs = []\n",
        "for i, txt in enumerate(train_output):\n",
        "  txt = \"[start] \" + txt + \"[end]\"\n",
        "  input_pairs.append((train_input[i], txt))\n",
        "\n",
        "print(len(input_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ki9aefYzDup",
        "outputId": "2bd652e9-c16f-4bc8-dd79-28d871c68786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "112000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the vocabulary size..."
      ],
      "metadata": {
        "id": "l4s8fsjEKfJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "input_words = set()\n",
        "for txt, _ in input_pairs:\n",
        "  for word in txt.split(\" \"):\n",
        "    if word not in input_words:\n",
        "      input_words.add(word)\n",
        "\n",
        "print(len(input_words))\n",
        "print(input_words)\n",
        "\n",
        "for _, txt in input_pairs:\n",
        "  for word in txt.split(\" \"):\n",
        "    if word not in input_words:\n",
        "      input_words.add(word)\n",
        "print(len(input_words))\n",
        "print(input_words)\n",
        "\n",
        "output_words = set()\n",
        "for _, txt in input_pairs:\n",
        "  for word in txt.split(\" \"):\n",
        "    if word not in output_words:\n",
        "      output_words.add(word)\n",
        "print(len(output_words))\n",
        "print(output_words)\n",
        "\n",
        "print(random.choice(input_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr4gqxMZz4ar",
        "outputId": "fc2513cb-8b64-437c-9e92-9fb17904ecc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "{'', 'g', 'a', 'f', 'b', 'h', 'd', 'e', 'c'}\n",
            "21\n",
            "{'', 'g', 'a', 'ee', 'm', '[end]', 'ef', 'd', 'eh', 'k', 'f', 'ed', 'b', 'l', 'h', 'i', 'c', 'eg', 'e', '[start]', 'j'}\n",
            "20\n",
            "{'g', 'a', 'ee', 'm', '[end]', 'ef', 'd', 'eh', 'k', 'f', 'ed', 'b', 'l', 'h', 'i', 'c', 'eg', 'e', '[start]', 'j'}\n",
            "('a d a d a e a d b d a d b d c f c f c g ', '[start] b d b d c f a d e f a d d g a e h c f a d i j c g a d k l [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 21 combined words from the 2 datasets, so ill just use vocab size 25 to be safe"
      ],
      "metadata": {
        "id": "zs7TpO9hwWTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "input_lengths = [len(txt.split(\" \")) for txt in train_input]\n",
        "output_lengths = [len(txt.split(\" \")) for txt in train_output]\n",
        "\n",
        "print(max(len(txt.split(\" \")) for txt in train_input))\n",
        "print(max(len(txt.split(\" \")) for txt in train_output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GgRaT1IxbBG",
        "outputId": "53cc3f62-96b9-480f-c299-a9e7cb4deab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33\n",
            "48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The max length of the sequences is 48, so ill use 48 as my sequence length."
      ],
      "metadata": {
        "id": "dne-7jHOK724"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use textvectorization to prepare the training data"
      ],
      "metadata": {
        "id": "Q1ZpRepjLDAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import keras.layers as layers\n",
        "import string\n",
        "import re\n",
        "\n",
        "strip_chars = string.punctuation\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "input_sequence_length = 48\n",
        "output_sequence_length = 48\n",
        "\n",
        "input_vocab_size = 25\n",
        "output_vocab_size = 25\n",
        "\n",
        "sequence_length = 48\n",
        "vocab_size = 25\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_source_texts = [pair[0] for pair in input_pairs]\n",
        "train_target_texts = [pair[1] for pair in input_pairs]\n",
        "source_vectorization.adapt(train_source_texts)\n",
        "target_vectorization.adapt(train_target_texts)"
      ],
      "metadata": {
        "id": "gLcNbxXsyK1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partition the data into training, validation, and test datasets"
      ],
      "metadata": {
        "id": "rJBujZ227wYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def format_dataset(source_text, target_text):\n",
        "  source = source_vectorization(source_text)\n",
        "  target = target_vectorization(target_text)\n",
        "\n",
        "  return ({ 'source' : source, 'target' : target[:, :-1] }, target[:, 1:])\n",
        "\n",
        "def make_dataset(input_pairs):\n",
        "  source_texts, target_texts = zip(*input_pairs)\n",
        "  source_texts = list(source_texts)\n",
        "  target_texts = list(target_texts)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((source_texts, target_texts))\n",
        "  dataset = dataset.batch(256)\n",
        "  dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "  return dataset.shuffle(256).prefetch(16).cache()\n",
        "\n",
        "random.Random(2048).shuffle(input_pairs)\n",
        "num_train_sequences = int(0.7 * len(input_pairs))\n",
        "num_validate_sequences = int(0.15 * len(input_pairs))\n",
        "\n",
        "train_dataset = make_dataset(input_pairs[:num_train_sequences])\n",
        "validation_dataset = make_dataset(input_pairs[num_train_sequences:num_train_sequences + num_validate_sequences])\n",
        "test_dataset = make_dataset(input_pairs[num_train_sequences + num_validate_sequences:])"
      ],
      "metadata": {
        "id": "z0Z_MVXU5FLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_dataset.take(1):\n",
        "  print(f'inputs[\"source\"] shape: {inputs[\"source\"].shape}')\n",
        "  print(f'inputs[\"target\"] shape: {inputs[\"target\"].shape}')\n",
        "  print(f'targets.shape: {targets.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajtbzrM67t2R",
        "outputId": "22c78a16-decb-49e0-ed17-283544cbcff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"source\"] shape: (256, 48)\n",
            "inputs[\"target\"] shape: (256, 48)\n",
            "targets.shape: (256, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the model I will use the PositionalEmbedding, TransformerEncoder, and TransformerDecoder layers provided by the textbook."
      ],
      "metadata": {
        "id": "PRQM35Ia5_pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "@keras.saving.register_keras_serializable(package=\"MyLayers\")\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "@keras.saving.register_keras_serializable(package=\"MyLayers\")\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "@keras.saving.register_keras_serializable(package=\"MyLayers\")\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = None\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ],
      "metadata": {
        "id": "R-O-ATuQ9BGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried using 2048 for the dense dimmension but I changed that to 1024 instead as that got better performance"
      ],
      "metadata": {
        "id": "PboBi-EYLjvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 1024\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"source\")\n",
        "x = PositionalEmbedding(input_sequence_length, input_vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"target\")\n",
        "x = PositionalEmbedding(output_sequence_length, output_vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(output_sequence_length, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "rgteEAdd8P2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bellow you can see the training process for the model"
      ],
      "metadata": {
        "id": "XlVpFbd7LtQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "save_model = keras.callbacks.ModelCheckpoint(\"Machine_Translation_Model.h5\", save_best_only=True, monitor=\"val_loss\")\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "transformer.fit(train_dataset, epochs=100, validation_data=validation_dataset, callbacks=[early_stopping, save_model])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OY1Kr0kAJyr",
        "outputId": "15aa3b17-f330-47c0-f45e-2edbbb9ce80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "307/307 [==============================] - 86s 264ms/step - loss: 1.6281 - accuracy: 0.4610 - val_loss: 0.8840 - val_accuracy: 0.6371\n",
            "Epoch 2/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "307/307 [==============================] - 80s 260ms/step - loss: 0.8712 - accuracy: 0.6481 - val_loss: 0.6379 - val_accuracy: 0.7317\n",
            "Epoch 3/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.6683 - accuracy: 0.7283 - val_loss: 0.5633 - val_accuracy: 0.7598\n",
            "Epoch 4/100\n",
            "307/307 [==============================] - 80s 261ms/step - loss: 0.5572 - accuracy: 0.7694 - val_loss: 0.4613 - val_accuracy: 0.8043\n",
            "Epoch 5/100\n",
            "307/307 [==============================] - 80s 261ms/step - loss: 0.4932 - accuracy: 0.7932 - val_loss: 0.4520 - val_accuracy: 0.8058\n",
            "Epoch 6/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.4445 - accuracy: 0.8108 - val_loss: 0.3983 - val_accuracy: 0.8250\n",
            "Epoch 7/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.4011 - accuracy: 0.8260 - val_loss: 0.4019 - val_accuracy: 0.8250\n",
            "Epoch 8/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.3783 - accuracy: 0.8353 - val_loss: 0.3789 - val_accuracy: 0.8340\n",
            "Epoch 9/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.3585 - accuracy: 0.8428 - val_loss: 0.3171 - val_accuracy: 0.8581\n",
            "Epoch 10/100\n",
            "307/307 [==============================] - 84s 274ms/step - loss: 0.3316 - accuracy: 0.8520 - val_loss: 0.3363 - val_accuracy: 0.8507\n",
            "Epoch 11/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.3298 - accuracy: 0.8544 - val_loss: 0.3794 - val_accuracy: 0.8324\n",
            "Epoch 12/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.3053 - accuracy: 0.8619 - val_loss: 0.2757 - val_accuracy: 0.8724\n",
            "Epoch 13/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.3031 - accuracy: 0.8640 - val_loss: 0.3186 - val_accuracy: 0.8567\n",
            "Epoch 14/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.2962 - accuracy: 0.8663 - val_loss: 0.2798 - val_accuracy: 0.8670\n",
            "Epoch 15/100\n",
            "307/307 [==============================] - 80s 261ms/step - loss: 0.2823 - accuracy: 0.8712 - val_loss: 0.2640 - val_accuracy: 0.8747\n",
            "Epoch 16/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.2736 - accuracy: 0.8736 - val_loss: 0.2779 - val_accuracy: 0.8695\n",
            "Epoch 17/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.2726 - accuracy: 0.8750 - val_loss: 0.2974 - val_accuracy: 0.8631\n",
            "Epoch 18/100\n",
            "307/307 [==============================] - 84s 274ms/step - loss: 0.2662 - accuracy: 0.8776 - val_loss: 0.3266 - val_accuracy: 0.8534\n",
            "Epoch 19/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.2560 - accuracy: 0.8816 - val_loss: 0.2463 - val_accuracy: 0.8836\n",
            "Epoch 20/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.2369 - accuracy: 0.8933 - val_loss: 0.2607 - val_accuracy: 0.8837\n",
            "Epoch 21/100\n",
            "307/307 [==============================] - 80s 261ms/step - loss: 0.2276 - accuracy: 0.9013 - val_loss: 0.1957 - val_accuracy: 0.9105\n",
            "Epoch 22/100\n",
            "307/307 [==============================] - 80s 261ms/step - loss: 0.2032 - accuracy: 0.9100 - val_loss: 0.1932 - val_accuracy: 0.9115\n",
            "Epoch 23/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.1928 - accuracy: 0.9155 - val_loss: 0.1629 - val_accuracy: 0.9268\n",
            "Epoch 24/100\n",
            "307/307 [==============================] - 84s 274ms/step - loss: 0.1724 - accuracy: 0.9243 - val_loss: 0.2001 - val_accuracy: 0.9133\n",
            "Epoch 25/100\n",
            "307/307 [==============================] - 84s 275ms/step - loss: 0.1706 - accuracy: 0.9281 - val_loss: 0.1715 - val_accuracy: 0.9228\n",
            "Epoch 26/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.1460 - accuracy: 0.9372 - val_loss: 0.1389 - val_accuracy: 0.9373\n",
            "Epoch 27/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.1474 - accuracy: 0.9395 - val_loss: 0.1620 - val_accuracy: 0.9348\n",
            "Epoch 28/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.1289 - accuracy: 0.9462 - val_loss: 0.1078 - val_accuracy: 0.9533\n",
            "Epoch 29/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.1220 - accuracy: 0.9502 - val_loss: 0.1014 - val_accuracy: 0.9566\n",
            "Epoch 30/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.1066 - accuracy: 0.9563 - val_loss: 0.1031 - val_accuracy: 0.9558\n",
            "Epoch 31/100\n",
            "307/307 [==============================] - 84s 274ms/step - loss: 0.1006 - accuracy: 0.9600 - val_loss: 0.0971 - val_accuracy: 0.9597\n",
            "Epoch 32/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0910 - accuracy: 0.9640 - val_loss: 0.0818 - val_accuracy: 0.9659\n",
            "Epoch 33/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.1079 - accuracy: 0.9619 - val_loss: 0.0722 - val_accuracy: 0.9700\n",
            "Epoch 34/100\n",
            "307/307 [==============================] - 84s 273ms/step - loss: 0.0760 - accuracy: 0.9707 - val_loss: 0.0564 - val_accuracy: 0.9777\n",
            "Epoch 35/100\n",
            "307/307 [==============================] - 84s 274ms/step - loss: 0.0659 - accuracy: 0.9745 - val_loss: 0.0667 - val_accuracy: 0.9739\n",
            "Epoch 36/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0625 - accuracy: 0.9765 - val_loss: 0.0731 - val_accuracy: 0.9726\n",
            "Epoch 37/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0595 - accuracy: 0.9783 - val_loss: 0.0927 - val_accuracy: 0.9651\n",
            "Epoch 38/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0550 - accuracy: 0.9799 - val_loss: 0.0358 - val_accuracy: 0.9859\n",
            "Epoch 39/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0481 - accuracy: 0.9825 - val_loss: 0.0350 - val_accuracy: 0.9860\n",
            "Epoch 40/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0443 - accuracy: 0.9840 - val_loss: 0.0482 - val_accuracy: 0.9826\n",
            "Epoch 41/100\n",
            "307/307 [==============================] - 84s 274ms/step - loss: 0.0428 - accuracy: 0.9850 - val_loss: 0.0302 - val_accuracy: 0.9889\n",
            "Epoch 42/100\n",
            "307/307 [==============================] - 84s 273ms/step - loss: 0.0379 - accuracy: 0.9865 - val_loss: 0.0438 - val_accuracy: 0.9833\n",
            "Epoch 43/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0400 - accuracy: 0.9864 - val_loss: 0.0239 - val_accuracy: 0.9907\n",
            "Epoch 44/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0347 - accuracy: 0.9881 - val_loss: 0.0351 - val_accuracy: 0.9877\n",
            "Epoch 45/100\n",
            "307/307 [==============================] - 79s 258ms/step - loss: 0.0323 - accuracy: 0.9888 - val_loss: 0.0405 - val_accuracy: 0.9853\n",
            "Epoch 46/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0290 - accuracy: 0.9901 - val_loss: 0.0223 - val_accuracy: 0.9913\n",
            "Epoch 47/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0281 - accuracy: 0.9907 - val_loss: 0.1128 - val_accuracy: 0.9600\n",
            "Epoch 48/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0294 - accuracy: 0.9903 - val_loss: 0.0243 - val_accuracy: 0.9908\n",
            "Epoch 49/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0245 - accuracy: 0.9916 - val_loss: 0.0182 - val_accuracy: 0.9930\n",
            "Epoch 50/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0227 - accuracy: 0.9922 - val_loss: 0.0299 - val_accuracy: 0.9898\n",
            "Epoch 51/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0228 - accuracy: 0.9925 - val_loss: 0.0152 - val_accuracy: 0.9942\n",
            "Epoch 52/100\n",
            "307/307 [==============================] - 84s 273ms/step - loss: 0.0235 - accuracy: 0.9926 - val_loss: 0.0156 - val_accuracy: 0.9944\n",
            "Epoch 53/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0185 - accuracy: 0.9938 - val_loss: 0.0163 - val_accuracy: 0.9940\n",
            "Epoch 54/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0199 - accuracy: 0.9935 - val_loss: 0.0159 - val_accuracy: 0.9942\n",
            "Epoch 55/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0177 - accuracy: 0.9942 - val_loss: 0.0145 - val_accuracy: 0.9947\n",
            "Epoch 56/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0172 - accuracy: 0.9945 - val_loss: 0.0212 - val_accuracy: 0.9925\n",
            "Epoch 57/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0174 - accuracy: 0.9943 - val_loss: 0.0124 - val_accuracy: 0.9953\n",
            "Epoch 58/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0148 - accuracy: 0.9951 - val_loss: 0.0125 - val_accuracy: 0.9954\n",
            "Epoch 59/100\n",
            "307/307 [==============================] - 79s 259ms/step - loss: 0.0130 - accuracy: 0.9958 - val_loss: 0.0117 - val_accuracy: 0.9956\n",
            "Epoch 60/100\n",
            "307/307 [==============================] - 79s 259ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0151 - val_accuracy: 0.9947\n",
            "Epoch 61/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0160 - accuracy: 0.9950 - val_loss: 0.0140 - val_accuracy: 0.9949\n",
            "Epoch 62/100\n",
            "307/307 [==============================] - 84s 273ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.0129 - val_accuracy: 0.9955\n",
            "Epoch 63/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0150 - accuracy: 0.9955 - val_loss: 0.0110 - val_accuracy: 0.9961\n",
            "Epoch 64/100\n",
            "307/307 [==============================] - 79s 259ms/step - loss: 0.0120 - accuracy: 0.9962 - val_loss: 0.0111 - val_accuracy: 0.9959\n",
            "Epoch 65/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0100 - accuracy: 0.9968 - val_loss: 0.0106 - val_accuracy: 0.9962\n",
            "Epoch 66/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0432 - accuracy: 0.9909 - val_loss: 0.0100 - val_accuracy: 0.9963\n",
            "Epoch 67/100\n",
            "307/307 [==============================] - 79s 259ms/step - loss: 0.0129 - accuracy: 0.9962 - val_loss: 0.0109 - val_accuracy: 0.9960\n",
            "Epoch 68/100\n",
            "307/307 [==============================] - 84s 273ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.0117 - val_accuracy: 0.9959\n",
            "Epoch 69/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0105 - accuracy: 0.9968 - val_loss: 0.0099 - val_accuracy: 0.9965\n",
            "Epoch 70/100\n",
            "307/307 [==============================] - 79s 259ms/step - loss: 0.0108 - accuracy: 0.9968 - val_loss: 0.0105 - val_accuracy: 0.9963\n",
            "Epoch 71/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0091 - val_accuracy: 0.9968\n",
            "Epoch 72/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0157 - accuracy: 0.9957 - val_loss: 0.0098 - val_accuracy: 0.9965\n",
            "Epoch 73/100\n",
            "307/307 [==============================] - 79s 259ms/step - loss: 0.0087 - accuracy: 0.9974 - val_loss: 0.0100 - val_accuracy: 0.9964\n",
            "Epoch 74/100\n",
            "307/307 [==============================] - 84s 273ms/step - loss: 0.0130 - accuracy: 0.9966 - val_loss: 0.0100 - val_accuracy: 0.9965\n",
            "Epoch 75/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.0085 - val_accuracy: 0.9970\n",
            "Epoch 76/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.0098 - val_accuracy: 0.9967\n",
            "Epoch 77/100\n",
            "307/307 [==============================] - 84s 272ms/step - loss: 0.0088 - accuracy: 0.9975 - val_loss: 0.0091 - val_accuracy: 0.9968\n",
            "Epoch 78/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0137 - val_accuracy: 0.9955\n",
            "Epoch 79/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.0082 - val_accuracy: 0.9972\n",
            "Epoch 80/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.0082 - val_accuracy: 0.9973\n",
            "Epoch 81/100\n",
            "307/307 [==============================] - 80s 259ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.0093 - val_accuracy: 0.9967\n",
            "Epoch 82/100\n",
            "307/307 [==============================] - 79s 259ms/step - loss: 0.0078 - accuracy: 0.9979 - val_loss: 0.0082 - val_accuracy: 0.9972\n",
            "Epoch 83/100\n",
            "307/307 [==============================] - 84s 273ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0088 - val_accuracy: 0.9972\n",
            "Epoch 84/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0088 - accuracy: 0.9977 - val_loss: 0.0081 - val_accuracy: 0.9974\n",
            "Epoch 85/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.0095 - val_accuracy: 0.9970\n",
            "Epoch 86/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.0081 - val_accuracy: 0.9972\n",
            "Epoch 87/100\n",
            "307/307 [==============================] - 84s 274ms/step - loss: 0.0205 - accuracy: 0.9955 - val_loss: 0.0102 - val_accuracy: 0.9965\n",
            "Epoch 88/100\n",
            "307/307 [==============================] - 80s 261ms/step - loss: 0.0088 - accuracy: 0.9975 - val_loss: 0.0080 - val_accuracy: 0.9972\n",
            "Epoch 89/100\n",
            "307/307 [==============================] - 84s 274ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0076 - val_accuracy: 0.9975\n",
            "Epoch 90/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.0090 - val_accuracy: 0.9970\n",
            "Epoch 91/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0084 - val_accuracy: 0.9973\n",
            "Epoch 92/100\n",
            "307/307 [==============================] - 84s 275ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0073 - val_accuracy: 0.9976\n",
            "Epoch 93/100\n",
            "307/307 [==============================] - 79s 258ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.0077 - val_accuracy: 0.9974\n",
            "Epoch 94/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0076 - val_accuracy: 0.9976\n",
            "Epoch 95/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0080 - accuracy: 0.9980 - val_loss: 0.0075 - val_accuracy: 0.9975\n",
            "Epoch 96/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0084 - val_accuracy: 0.9976\n",
            "Epoch 97/100\n",
            "307/307 [==============================] - 80s 260ms/step - loss: 0.0062 - accuracy: 0.9984 - val_loss: 0.0079 - val_accuracy: 0.9976\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ef1873a02e0>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we can see the model is working very well and we are able to predict the next token with 99.76% validation accuracy"
      ],
      "metadata": {
        "id": "bWhuIL39fgQH"
      }
    }
  ]
}